> 트랜스포머 모델은 다방면에서 활용되어진다.  
이 페이지는 그중 비전 트랜스포머(ViT)를 구현한 것이다.  
코드는 아래의 코랩에 기입해두었다.  
![Google Colab](https://img.shields.io/badge/googlecolab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)  
https://colab.research.google.com/drive/1D6sc47P0yr9a6uKWy2z2kTP3_OKLFSkM?usp=sharing

# 사용 데이터
>roboflow - TrafficSignClassification  
https://universe.roboflow.com/tcc-12krs/trafficsignclassification-fj6x0

해당 모델을 훈련하기 위해 사용된 데이터이다.  
교통 표지판 분류 문제이며 그레이스케일, 640x640x3의 크기를 가지고 있다. 사전에 50% 상하좌우 뒤집기 증대(augmentation)가 적용되어있다. 총 클래스 라벨은 0부터 42까지 43개이다.  
![](https://velog.velcdn.com/images/swoo64/post/83f5a533-7725-4693-a4f8-5f44707ca35e/image.png)

## 전처리
학습을 위해 이미지를 224x224 크기로 줄였다.  
또한 모델에 데이터를 입력하기 전 개인적으로 augmentation을 추가 적용하였다. 이것은 train 데이터셋에만 적용했는데, validation과 test의 경우 모델의 성능을 객관적으로 평가해야 하기 때문이다. 
- 최대 15도 회전
- 가로세로 최대 10% 이동
- 크기 최대 10% 확대축소
- 색상, 밝기, 대비 각각 최대 30%, 30%, 20% 변환

# I. ViT 구성
## 1. Patch Embedding
이미지 처리 트랜스포머도 자연어 처리 트랜스포머와 같은 방식을 공유한다. 자연어에선 각 문장마다 각각의 단어(토큰)들이 있었다. 이미지에선 각 사진마다 토큰을 만들어줘야 한다. 즉 이미지를 작은 패치(Patch)조각들로 나누고, 하나의 패치를 벡터의 형태로 변환하여야 한다.  
<br>
1. 입력의 크기는 `(Batch = 32, Chennel = 3, Height = 224, Weight = 224)`이다.  
그레이스케일 이지만 ViT와 CNN은 이미지를 3채널로 입력받는걸 전제로 하기 때문에 채널은 RGB 3개이다.
2. 이것을 `16x16`의 패치로 슬라이싱을 하면 14*14=196개의 패치가 각각의 이미지에 생성된다.
3. Flatten : 각 패치를 1차원 벡터로 변환한다. 이때 채널 수가 3개이므로 16*16*3=768차원이다. `(32, 196, 768)`
4. 그러나 차원수가 너무 커서 선형변환을 통한 차원 축소를 진행한다. -> `(32, 196, 384)`
5. 이때 `Conv2d`메서드를 사용하면 패치와 선형변환을 한번에 진행할 수 있다.

## 2. ViT 입력 준비
앞서 만든 Patch Embedding을 이용하여 ViT모델에 입력할 데이터를 준비한다. 여기에선 입력 시퀸스 앞에 CLS토큰을 하나 추가하여 해당 시퀸스 전체를 대표하여 요약하는 토큰을 생성한다. 각각의 이미지에 하나씩 존재하며 그 크기는 `(1, 1, 384)`이다.  
또한 위치 임베딩을 생성하여 입력 시퀸스에 더하는데 그 크기는 `(1, 1+196, 384)`이다.

## 3. 트랜스포머 인코더 블록
트랜스포머에선 동일한 블록을 여러개 쌓아 스택을 이룬다. 따라서 하나의 블록에 대해 설계만 하여도 된다. 

1. 셀프 어텐션에 시퀸스를 입력하기 전에 층 정규화를 적용한다. (사전 층 정규화)
2. 멀티헤드 셀프 어텐션을 사용한다. 파이토치에선 `MultiheadAttention` 메서드를 통해 이를 쉽게 구현할 수 있더, 해드의 개수는 8개로 사용하였고, 해당 메서드의 입력 순서는 `(N, B, Embed)`이다. 따라서 `batch_first = True` 옵션을 이용하여 우리의 데이터에서 배치의 위치에 맞게 설정한다.
3. 이후 셀프 어텐션을 적용하기 전의 시퀸스를 이용하여 잔차 연결을 한다.
4. FFNN을 적용하기 전 층 정규화를 한번 더 진행하고, FFNN의 은닉층의 차원은 입력층의 4배(여기선 384*4개), 활성화 함수는 `GELU`를 사용하여 출력층은 다시 원래의 384로 차원 축소를 한다.
5. 마지막으로 FFNN 입력 전 시퀸스를 가지고 잔차 연결을 진행한다.  
6. 이렇게 해서 하나의 인코더 블록이 완성되었다.

## 4. 트랜스포머 인코더
위에서 만든 인코더 블록을 여러개 쌓아 스택을 만든다.
여기선 6개의 블록을 쌓았다.  
입력 시퀸스가 모든 블록을 지난 후 마지막 층 정규화를 진행한다.

## 5. 분류 헤드
인코더 블록 스택을 지나온 이미지 임베딩 행렬에서 맨 앞의 CLS토큰만 추출한다. 이미지 패치 벡터는 분류에 사용되지 않고, 오직 CLS를 학습시키는 데만 이용된다. 이어서 이것을 클래스 라벨 43개의 차원으로 선형변환을 한다. 그 결과 각 클래스별로 로짓 점수가 생성된다. 소프트맥스 함수는 학습 과정에서 적용한다.

# II. ViT 모델 생성
생성된 모델의 구조는 다음과 같다.
```python
ViTClassifier(
  (vit_input): ViTInput(
    (patch_embed): PatchEmbedding(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): TransformerEncoder(
    (blocks): ModuleList(
      (0-5): 6 x EncoderBlock(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
        )
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=1536, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=1536, out_features=384, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (final_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (head): Linear(in_features=384, out_features=43, bias=True)
)
```

summary로 보면 다음과 같다.
```
==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
ViTClassifier                                                     --
├─ViTInput: 1-1                                                   76,032
│    └─PatchEmbedding: 2-1                                        --
│    │    └─Conv2d: 3-1                                           295,296
│    └─Dropout: 2-2                                               --
├─TransformerEncoder: 1-2                                         --
│    └─ModuleList: 2-3                                            --
│    │    └─EncoderBlock: 3-2                                     1,774,464
│    │    └─EncoderBlock: 3-3                                     1,774,464
│    │    └─EncoderBlock: 3-4                                     1,774,464
│    │    └─EncoderBlock: 3-5                                     1,774,464
│    │    └─EncoderBlock: 3-6                                     1,774,464
│    │    └─EncoderBlock: 3-7                                     1,774,464
│    └─LayerNorm: 2-4                                             768
├─Linear: 1-3                                                     16,555
==========================================================================================
Total params: 11,035,435
Trainable params: 11,035,435
Non-trainable params: 0
==========================================================================================
```

## 6. 손실함수와 옵티마이저
손실 함수는 크로스 엔트로피를 사용하고, 옵티마이저는 AdamW를 이용한다. 이때 트랜스포머 모델의 학습 가능한 모든 파라미터를 입력한다.

## 7. Warm-up 및 스케줄러
AdamW의 학습률을 1e-4로 설정하였더. 그러나 학습 초기 에포크에선 이것조차 큰 값일 수 있다. 따라서 초기 5개의 에포크에선 웜업, 준비운동의 개념으로 해당 학습률을 줄인다. (첫번째 에포크에서 학습률은 1e-4*(1/5))

# III. 학습 결과
50에포크동안 학습을 진행하였다.  
최종 테스트 정확도는 0.82가 나왔다.  
손실, 정확도 곡선은 아래와 같다.  

![](https://velog.velcdn.com/images/swoo64/post/6e8556a7-b19b-4001-88a0-4fc58e9190f4/image.png)
![](https://velog.velcdn.com/images/swoo64/post/4c5eb661-6e5b-456d-82b5-5bd0b18568e8/image.png)

에포크가 진행될수록 점점 모델이 성능이 좋아진다. 50에포크 이상 학습을 진행하여도 수렴이 될 것으로 보여진다. 그러나 ResNet-101같은 경우 같은 데이터셋에 진행한 결과 정확도가 0.98로 아주 완벽한 결과를 보여주었다. 비전 트랜스포머 모델같은 경우 이미지 내의 각 픽셀, 패치에 대해 문맥을 설명해주는 것이여서, 단순 표지판 분류 문제에는 이미지의 기하학적 특징을 잘 파악하는 CNN이 더욱 어울리는 것처럼 보인다.  
흔히 트랜스포머 모델을 사용할 경우 방대한 학습이 필요하기에 사전 학습 모델을 이용하는 것이 반필수로 여겨진다. 그러나 해당 데이터의 경우 모델을 처음부터 학습했음에도 불구하고 정확도가 0.82 언저리에 도달할 정도로 준수한 성능을 보여주었다. 

# Reference
- ![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=GitHub&logoColor=white)
- [3. Vision_Transformer.md](https://github.com/Max-JI64/Today-I-Learn/blob/main/Study/Computer_Vision/3.%20Vision_Transformer.md)
- ![Google Colab](https://img.shields.io/badge/googlecolab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)
- https://colab.research.google.com/drive/1D6sc47P0yr9a6uKWy2z2kTP3_OKLFSkM?usp=sharing