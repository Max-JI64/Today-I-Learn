>트랜스포머 아키텍처 및 챗GPT 동작 원리  
https://cogdex-dtta.streamlit.app/  
해당 문서애서는 위의 링크의 설명에 기반하지만, 실습은 컴퓨터 비전의 트랜스포머에 적용하였다.  


# 트랜스포머란?
트랜스포머(Transformer)는 자연어 처리에 사용되는 모델 중 하나이다. 기존의 RNN과 같은 모델은 각 토큰이 등장한 순서를 기준으로 순차대로 학습 및 예측을 진행한다. 이를 통해 앞뒤 문맥을 읽어낼 수 있었다. 그러나 두가지 문제점이 발생한다. 첫번째는 장기 의존성 문제로, 문장의 길이와 토큰의 순서가 길어질수록 기울기 소실이 발생, 과거 정보가 희미해지며 긴 문장을 학습하기 어렵다. 또한 이전 시점의 계산이 끝나야 다음 시점을 계산할 수 있는 순차적 계산의 비효율성으로 인해 병렬화가 어렵고 학습 속도가 느리다.

## 트랜스포머의 장점
트랜스포머 모델은 RNN과 마찬가지로 문장의 문맥을 고려한다. 그러나 토큰의 순서와는 관계 없이, 모든 단어를 **동시에 병렬 처리**하기 때문에 GPU 병렬 연산을 극대화 하며 긴 문장에서도 안정적으로 학습이 가능하다. 즉 모든 단어 쌍의 관계를 한번에 계산하기 때문에, 긴 거리의 의존성을 해결할 수 있다. (한 단어와 연관되어있는 다른 단어가 서로 문장 끝에 있으면, RNN은 서로 연관이 적은 것으로 간주한다.)
이러한 특성을 이용하여 트랜스포머는 NLP뿐만이 아닌 이미지(ViT), 음성, 시계열, 영상 등 수많은 딥러닝 분야로 확장되었다.

# 트랜스포머의 구조
## 트랜스포머 인코더
![](https://velog.velcdn.com/images/swoo64/post/ae544285-bc40-421b-a498-2a3b5b457664/image.png)

트랜스포머 인코더는 입력 문장을 이해하기 위한 **문맥을 해석**한다.  
하나의 인코더는 여러개의 인코더 블록이 순차적으로 쌓여있는 구조를 가진다.  
하나의 인코더 블록에는 멀티 헤드 셀프 어텐션과 잔차 연결, 층 정규화, 피드 포워드 뉴럴넷(FFNN)이 서로 연결되어있다.  

### 토큰화와 소스의 입력 임베딩
![](https://velog.velcdn.com/images/swoo64/post/2894ee23-461e-4e4b-b408-0afcc85d6383/image.png)
자연어로 이루어진 글은 컴퓨터가 이해할 수 없다. 따라서 글자를 숫자로 바꾸어주는 과정이 필요하다. 이때 토큰화(Tokenization)는 문장을 단어, 서브단어 등의 단위로 쪼갠다. 그 결과 각 단어는 토큰화되어 고유의 정수값을 ID로 갖는다.   
임베딩은 단어를 고정된 길이의 벡터로 바꿔주는 역할을 한다. 각각 정수 ID로 매핑된 토큰으론 학습이 불가능 하다. 따라서 각 단어마다 고차원의 벡터로 변환해 준다. 각 토큰의 ID를 기준으로 원핫 벡터를 만든 다음, 이것을 사전에 훈련된 임베딩층에 통과시킨다. 그러면 임베딩 층의 가중치 행렬의 특정 행을 각 토큰별로 얻을 수 있다.  
한 문장의 단어가 `N`개이고 임베딩 차원 수가 `D`일때 소스의 입력 임베딩의 크기는 `(NxD)`이다.

### 위치 인코딩과 입력 임베딩
![](https://velog.velcdn.com/images/swoo64/post/ad155111-b7bf-4a3a-a03f-fb789af8a4d8/image.png)

기존의 RNN기반의 모델에선 토큰의 입력 순서가 중요하다. 그러나 이것은 서로 연관이 있지만 멀리 떨어저 있는 두 단어는 큰 상관이 없다고 판단해버린다.  
트랜스포머 모델은 입력 토큰의 순서를 고려하지 않는다. 대신 한 문장을 한번에 병렬처리한다. 이때 위치 인코딩을 더해주면, 각 토큰은 시퀸스 내의 위치에 대한 고유 식별자를 얻을 수 있다. 이를 통해 병렬처리와 위치 기반 학습이 가능해진다.   

사인, 코사인을 이용한 위치 임베딩의 공식은 아래와 같다.  
$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i / d_{model}}}\right)$  
$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i / d_{model}}}\right)$  
`pos`는 각 토큰의 순서대로 0부터 N-1까지 증가한다. 홀수 인덱스 차원에는 $sin$, 짝수 인덱스 차원에는 $cos$이 들어간다.  
이 공식에 주의할 점은 각 차원별로 파동의 주기가 다르게 만들어진다는 것이다. 낮은 차원은 주기가 짧아 미세한 위치 변화에 민감하고, 높은 차원은 주기가 길어 전체적인 위치 패턴을 인식한다. 이를 통해 한 문장 안의 위치를 여러 스케일로 인코딩하는 효과를 얻을 수 있다.   
<br>
토큰 임베딩은 소스의 입력 임베딩에 위치 임베딩을 원소별 덧셈을 한 결과이다. 그 결과 토큰 임베딩은 각 토큰과 문장의 의미 정보와 위치에 대한 패턴을 갖고 있다. 이후 셀프 어텐션 매커니즘을 통해 내용과 위치에 따라 서로 다른 토큰의 중요도(유사도, 관련성)를 평가하여 문맥을 인식한다. (위치 임베딩은 각 토큰의 의미에 대한 정보가 들어있지 않다. 오로지 원시 위치 정보만 갖고있다.)  
<br>
각 임베딩 행렬의 크기는 전부 `(NxD)`로 동일하다.  

### 인코더 블록
![](https://velog.velcdn.com/images/swoo64/post/055c55db-89a9-434a-bffd-9ab2918b6cbb/image.png)  
앞서 언급하였듯 하나의 인코더 블록에는 멀티 헤드 셀프 어텐션과 잔차 연결, 층 정규화, 피드 포워드 뉴럴넷(FFNN)이 서로 연결되어있다.

#### 멀티 헤드 셀프 어텐션
> 여러개의 셀프 어텐션을 병렬처리하는 것을 멀티 헤드 셀프 어텐션이라 한다.  
이를 통해 다양한 관점에서 해당 문장을 바라볼 수 있다.

언어 모델의 기본 원칙은 "단어의 의미는 주변 단어에 의해 세밀하게 조절되거나 변화된다"이다. 단어 자체엔 기본 의미가 있지만, 그 단어가 사용되는 맥락이 해당 단어에 더욱 구체적인 의미를 형성해준다. 언어 모델은 주변 단어들 사이의 관계를 인식하며 위미(semantic)정보, 구문(syntactic)정보, 문맥(contextual)정보를 파악한다.   
셀프 어텐션 메커니즘은 문장 내 모든 단어들간의 관계를 동시에 고려하여 문맥에 따른 단어의 의미를 파악한다. 이것은 현재의 토큰이 자기 자신과, 자신 이전 및 이후에 있는 토큰들에 관심을 기울이게 함으로서 모든 단어들 사이의 문맥적 정보를 반영하며 유사도를 계산한다. 즉 **각각의 토큰 자체**에 입력 시퀸스내의 다른 모든 토큰들과의 관계 정도에 대한 정보가 반영된다는 것이다.  
<br>
> 아래의 셀프 어텐션 수행 과정은 토큰수 `N=8`, 차원수 `D=512`를 예시로 든것이다.  

![](https://velog.velcdn.com/images/swoo64/post/69213b25-6d2a-4d59-ac41-27d66a26919a/image.png)


셀프 어텐션은 Q, K, V 벡터를 학습해나간다.  

1. Q : Query 벡터  
현재 처리중인 토큰을 나타내며, 다른 토큰으로부터 정보나 문맥을 얻으려 한다  
'다음 단어를 이해하거나 예측하기 위해 어떤 토큰에 집중해여 하는가?'

2. K : Key 벡터  
Q 벡터가 찾고있는 토큰, 문장에 있는 모든 토큰의 K 벡터는 일종의 '인덱스 및 라벨'역할을 한다  
각 토큰은 K벡터를 통해 현재 처리중인 Q벡터와의 잠재적 관련성을 알린다  
셀프 어텐션에서 모든 토큰은 Q이자 K의 역할을 하며, 이는 다른 모든 토큰으로부터 정보를 얻으며 제공한다는 의미이다

3. V : Value 벡터
계산된 어텐션 가중치를 통해 문장의 각 Q와 K와의 관련성 정도가 얻어지면, 모델은 관련 토큰들의 V벡터를 검색한다  
즉 관련성이 높을수록 해당 V벡터에 치중한다.  
여기엔 각 토큰의 정보를 포함하고 있다.  
각 가중치는 V벡터에 부여되고 최종적인 가중합이 해당 어텐션 계층의 출력이다.  
<br>
`1번째 토큰`에 대한 예시는 아래와 같다.  
![](https://velog.velcdn.com/images/swoo64/post/bf912e67-ba9d-42ab-9845-af1f40db4719/image.png)  
`1번째 토큰`에 대한 임베딩 벡터는 Q, K, V의 가중치 행렬 $W^Q$, $W^K$, $Q^V$와 각각 행렬곱을 한다. 그 결과 `1번째 토큰`에 대한 Q, K, V 벡터 $Q_1$, $K_1$, $V_1$가 생성된다. 이때 Q, K, V의 각 가중치 행렬의 `64`차원(**k**)은 임베딩 행렬의 차원을 셀프 어텐션의 헤드 수로 나눈 값이다. (본 예시에서 헤드 수는 8개이므로 512/8 = 64이다.)  

![](https://velog.velcdn.com/images/swoo64/post/fb3b6929-23e6-4f25-a704-42f837548c33/image.png)  
이후 $Q_1$와 $K_1$벡터는 서로 내적곱을 진행한다. (K벡터를 전치행렬 $K^T$로 변환한다.) 이후 결과값인 각각의 스칼라에 Q, K의 차원 수(**k**)의 제곱근으로 나눈다. 이것은 연산량을 줄이기 위한 것으로 Scaled dot Product라고 불린다.   
그 결과 각각의 결과에 대해 Attention Score이 생성되는데 두 토큰간의 관계성이 클수록 그 값이 커진다. 왜냐하면 코사인 유사도 공식을 통해 벡터간 유사도는 두 벡터의 내적값에 비례하기 때문이다.  
<br>
$similarity = \cos(\Theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^{n} A_i \times B_i}{\sqrt{\sum_{i=1}^{n} (A_i)^2} \times \sqrt{\sum_{i=1}^{n} (B_i)^2}}$  
여기서 $\sum_{i=1}^{n} A_i \times B_i$이 벡터의 내적이다.  

![](https://velog.velcdn.com/images/swoo64/post/61a14975-ce6e-482c-8294-aafd4428f020/image.png)  


이후 실질적인 유사도 점수를 얻기 위해 Attention Score을 $softmax$함수에 통과시킨다. 그 결과로 생성되는 Attention Weight는 총 합이 1이며 각각의 값이 두 토큰간의 유사도로 명확히 보여진다. 이후 각각의 Attention Weight는 해당 V벡터와 곱해지고, 그 결과는 서로 합해진다. 그 결과 최종적으로 `1번째 토큰`에 대한 $Z_1$벡터를 얻을 수 있다. 이것은 `1번째 토큰`과 문장에 속한 각 토큰과의 문맥적 표현을 담고 있다. 셀프 어텐션 메커니즘은 해당 과정을 모든 토큰에 대해 반복하여 Z행렬를 생성한다.  
<br>
셀프 어텐션 메커니즘의 모든 토큰에 대한 과정을 하나의 도식도에 나타내면 다음과 같다.  
![](https://velog.velcdn.com/images/swoo64/post/531b04d3-3560-4084-be6e-10e03da38bbd/image.png)  
셀프 어텐션 메커니즘의 결과물인 $Z_h$는 h번째 셀프 어텐션의 Z행렬를 나타낸다. h번의 멀티 헤드 어텐션을 병렬처리하면 아래와 같이 $Z_1$, $Z_2$, ..., $Z_h$ h개의 Z행렬을 얻을 수 있다.  
![](https://velog.velcdn.com/images/swoo64/post/905666b6-6a7f-424d-ad9f-8507b3c20875/image.png)  
k차원은 D차원을 h개의 멀티 헤드로 나눈 값이다. 따라서 모든 Z행렬을 이어 붙이면(concatenate) 최종 Z행렬을 얻을 수 있다.  
<br>
![](https://velog.velcdn.com/images/swoo64/post/caf0932b-596a-4abf-b322-1b7b1d8027bb/image.png)  

이후 연결된 Z행렬을 출력 가중치 행렬 $W^0$와 곱한다. 이것의 크기는 임베딩 행렬의 차원 크기 D로 이루어진 `(DxD)`이다. 출력 가중치 행렬을 곱하는 이유는 여러 헤드에 걸쳐 정보를 혼합하기 위한 선형 변환이다. $W^0$는 훈련 중 계속 업데이트 되어지며 여러 헤드의 정보를 가장 적합한 방식으로 결합하는 데 도움을 준다. 이것이 없으면 각 헤드가 학습한 정보는 개별적으로 유지되어진다. 그 결과로 최종 멀티 헤드 셀프 어텐션 행렬이 출력된다.  
<br>
![](https://velog.velcdn.com/images/swoo64/post/dc87dfa9-3bcf-4d03-87be-eb0b46db2adf/image.png)    
이것은 입력된 토큰 임베딩과 같은 `(NxD)`크기이다. 최종 결과물 멀티 헤드 셀프 어텐션 행렬은 각 토큰의 초기 의미 뿐만 아니라 전체 시퀸스 내의 문맥(토큰 사이의 관계)까지 포착한다.   



#### 잔차 연결
> 본 예제에선 초기의 트랜스포머 모델로, 각각 셀프 어텐션과 FFNN 이후에 연결된다(사후 연결). 그러나 최신 모델은 사전 연결을 이용한다.  

잔차 연결이란 계층을 통과하는 원래 경로에 우회 경로를 하나 추가하는 것이다.  특히 ResNet의 회귀 연결에서 사용하는 방법과 유사하다. 셀프 어텐션 이후의 잔차 연결의 경우, 멀티 헤드 셀프 어텐션 행렬에 토큰 임베딩 행렬을 각 원소끼리 값을 더한다. 이를 통해 모델은 원래의 의미 정보를 유지하면서, 셀프 어텐션을 통해 얻은 문맥적 정보도 통합할 수 있다.   
그 결과 셀프 어텐션 메커니즘에서 잠재적으로 손실되거나 희석될 수 있는 정보를 후속 레이어에서 계속 사용할 수 있다. 또한 시퀸스의 각 토큰에 대한 보다 포괄적인 표현을 생성할 수 있다. 마지막으로 역전파 과정 중 그레디언트를 앞의 층에 바로 전달할 수 있으며 항상 1의 기울기가 유지되기 때문에 기울기 소실 문제를 완화할 수 있다. (x를 미분하면 1)


#### 층 정규화
각각의 층(행, 토큰)에 대하여 평균과 표준편차를 구한 후 정규화를 진행한다. 해당 공식은 아래와 같다.  
$LayerNorm(x_{i,k}) = \gamma \times \frac{x_{i,k} - \mu_{i}}{\sqrt{\sigma_{i}^{2} + \epsilon}} + \beta$  
- $\epsilon$은 분모가 0이 되어주는 것을 방지하는 작은 값이다
- $\gamma$와 $\beta$는 학습 가능한 파라미터이다.
<br>
이를 이용해 정규화된 데이터를 스케일링 및 시프팅 한다.
<br>
층 정규화는 활성화 함수를 통과하기 전에 진행하면 모델의 학습을 안정화하고 가속화한다.(모델의 깊이가 깊어져도 그레디언트 계산이 잘 진행될 수 있다.) 트랜스포머 아키텍처에서는 주로 잔차 연결 전후로 적용된다. 최근의 많은 모델에선 잔차 연결을 수행하기 전 정규화(사전 층 정규화)를 적용한다.   

#### 포지션 와이즈 피드 포워드 뉴럴넷 FFNN
![](https://velog.velcdn.com/images/swoo64/post/40191536-4252-40dc-a79b-aac60e79af63/image.png)
멀티 헤드 셀프 어텐션 행렬은 FFNN에 입력되어 차원 확장, 비선형성, 차원 축소의 과정을 거치면서 데이터 내의 복잡한 패턴과 관계를 학습할 수 있다. 행렬의 각 행(토큰의 임베딩 벡터)에 독립적으로 작용된다.(포지션 와이즈 position-wise)  

입력 토큰 임베딩(어텐션 행렬)이 FFNN에 입력되면 첫 층에서 선형 변환을 거친다. 첫번째 입력층은 임베딩 벡터의 D차원을 가진다. 이후 은닉층에선 해당 차원이 4배로 증가하며 특징 공간의 확장이 일어난다.(경험적으로 4배가 적당하다고 함) 이후 다시 원래 D차원으로 차원 축소가 진행된다.  
은닉층에선 활성화 함수를 통해 비선형 변환이 일어나는데, 주로 ReLU 혹은 GELU가 사용된다. 

### Encoder Output
최종적으로 생성된 임베딩 벡터이다. 처음에 입력된 소스의 입력 임베딩과 같은 `(NxD)`크기이다. 처음엔 해당 토큰의 기본적 의미만을 담고 있었지만, 셀프 어텐션 계층을 거치면서 입력 시퀸스에서의 다른 토큰들과의 관계 - 문맥적 의미를 담는 임베딩 벡터로 진화하고, FFNN을 거치면서 토큰에 대해 포착된 더 다양한 특징들, 토큰 사이의 복잡한 관계와 패턴 정보까지 담고 있는 임베딩 벡터로 변화한다.

## 트랜스포머 디코더
> 여기선 디코더의 자세한 작동 원리가 아닌, 인코더와 비교하여 디코더에 새로 추가된 메커니즘만을 추가한다.   
<br>
![](https://velog.velcdn.com/images/swoo64/post/b962c31c-f42c-42b5-835d-7ed904ab196f/image.png)  
인코더에서는 소스 문장이 입력되어지고, 입력 문장의 문맥 정보를 담고있는 임베딩 벡터가 출력된다. 즉 입력 문장의 의미를 이해하고 전체 문맥을 압축한 벡터로 표현되어진다.   
디코더는 타겟문장, 예를 들어 변역 결과의 앞부분을 입력받고 다음 단어의 확률 분포를 출력한다. 여기선 앞(미래)의 단어를 보지 못하게 마스킹 처리를 하고, 인코더의 출력을 참고하여 입력과의 대응 관계를 학습한다.   

### 마스크 멀티 헤드 어텐션
인과적(causal) 셀프 어텐션이라고도 불린다. 즉 원인이 되는 정보만을 사용하여 그 결과를 예측할려는 것이다. 셀프 어텐션에선 현재의 토큰이 자신 이전에 있는 토큰(**원인**)에만 관심을 기울이고, 자신 이후의 토큰(**결과**)에는 관심을 기울이지 않는 것을 말한다. 이렇게 함으로서 각 단어 토큰은 자신 이전까지의 문맥 정보를 담을 수 있어, 다음 단어를 예측할 때 문장 내의 단어들 간의 순차적인 의존성을 반영하고 의미론적으로 일관된 텍스트를 생성할 수 있다. 

#### 마스크
인과적 셀프 어텐션은 인코더에서 진행한 셀프 어텐션과 동일한 방법을 사용한다. 즉 모든 토큰에 대해 관계를 계산하는 것은 동일하다는 것이다. 하지만 우리는 현재 토큰 이후의 토큰과의 관계를 계산하면 안된다. 이때 사용되는 것이 마스크 행렬이다.  
![](https://velog.velcdn.com/images/swoo64/post/f7c51645-14e2-4de6-b1af-e4b15309f6fc/image.png)  
마스크 행렬은 어텐션 점수 행렬과 같은 크기를 가지며, 하삼각 행렬(대각선 아래)의 원소들은 0의 값을 갖고 있으며, 상삼각 행렬은 마이너스 무한대 값을 갖고 있다. 이때 하삼각 행렬은 자신과 이전 시점, 상삼각 행렬은 미래 시점을 나타낸다.  
두 행렬을 더하면, 마스킹된 어텐션 점수 행렬의 하삼각 행렬은 원래 어텐션 점수 행렬과 같은 값을 가지지만, 상삼각 행렬은 마이너스 무한대의 값을 가지게 된다.   

![](https://velog.velcdn.com/images/swoo64/post/2435846e-af73-475d-bd4e-e18b5f2277ac/image.png)  
이것이 소프트맥스 함수를 지나고 어텐션 가중치를 계산하면, 현재 토큰 이후의 어텐션 가중치의 값은 거의 0에 가까워진다. 따라서 이후 V행렬을 곱하게 되더라고 이에 해당되는 토큰들의 Z행렬은 거의 영향을 주지 못하게 된다. 이를 통해 미래 시점의 토큰들의 영향력은 해당 과정에 의미가 없게 되어진다.

### 트랜스포머 스택
트랜스포머 아키텍처에서는 동일한 트랜스포머 블록이 여러개 쌓여 스택을 이루고 있다. 임베딩 벡터가 여러개의 블록을 통과하면서 더욱 풍부한 문맥 정보와 토큰간의 관계를 반영하여 더 싶은 수준으로 진화한다. 또한 상위 블록으로 갈수록 인과적 셀트 어텐션과 FFNN의 비선형성에 노출되며 임베딩 벡터는 정제되어진다. 즉 점점 고차원적인 패턴을 파악하게 되면 '추상화'가 진행된다. '추상화'과정은 복잡한 정보 속에서 필요한 것만을 식별하고 이해하는 능력을 말한다. 

### 예측층 - 선형 변환
임베딩 벡터가 마지막 트랜스포머 스택을 지난 후 선형 변환을 적용한다. 이것은 임베딩 벡터의 D차원을 입력받고, 모든 단어 사전의 개수, 해당 언어의 어휘 크기인 Vocab차원을 출력한다. 

### 예측층 - 소프트맥스
선형 변환의 결과물인 행렬의 각 행의 원소는 로짓 점수를 갖고 있다. 만약 해당 언어의 어휘 집합에 단어 50,000개가 들어있다면, `Vocab` = 50,000개의 로짓 점수를 반환한다. 이것을 소프트맥스 함수에 입력하면, 각각의 단어가 해당 토큰의 다음에 위치할 확률이 나타나진다.


# Reference
- https://cogdex-dtta.streamlit.app/
- ![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=GitHub&logoColor=white)
- [2. Transformer.md](https://github.com/Max-JI64/Today-I-Learn/blob/main/Study/Computer_Vision/2.%20Transformer.md)

