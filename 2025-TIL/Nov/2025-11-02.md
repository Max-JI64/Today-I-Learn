# TIL

## Date: 2025-11-02

## Today I Work

### Work 1: Resnet Fine-Tuning
#### 서론
 파인튜닝(Fine-Tuning)은 이미 대규모 데이터로 학습된 모델의 성능을 이용하는 것이다. 그 이유는, 해당 모델은 나 자신이 작업하는 도메인에 대해 충분한 학습이 안되어있을 수 있고, 적은 양의 데이터로는 모델은 처음부터 충분히 학습시킬 수 없기 때문이다. 따라서 기존 모델의 언어와 이미지로부터 학습한 패턴을 이용하여, 자신에게 적합한 모델로 개조하는 것에 가깝다. 이로 인해 데이터 부족으로 인한 과적합을 줄이고 효율성을 늘릴 수 있다.
#### 데이터셋 설명
 CIFAR-10 이미지 데이터를 이용하여 파인튜닝을 진행할 것이다. 이것은 10개의 클래스로 구분되어 있으며, 각 이미지는 32x32x3 크기이다. train : validation : test = 40000, 10000, 10000개로 분할하였다.
#### 모델 설명
 파인튜닝 대상이 되는 모델은 ResNet18이다. ResNet은 잔차 신경망(Residual neural Network)으로 이루어졌다. 기존의 깊은 신경망은 기울기 소실에 의해 성능이 무한정 좋아지진 않기 때문에 이걸 극복하기 위해서이다. 여기엔 잔차 학습이 사용되는데, 모델이 출력 그 자체를 학습하는 것이 아닌, 입력과 출력간의 잔차를 학습한다. 이것은 각 잔차 블록마다 이루어지며, 잔차에 입력값을 더하는 형태로 출력한다. 따라서 모델은 잔차를 0으로 만드는 방향으로 학습을 진행하며, 이를 통해 불필요한 경우엔 변환을 하지 않아 학습 효율성과 안정성을 높일 수 있다.
 ResNet18은 레이어가 18개이다. Conv1 7×7 conv 1개, Conv2_x ~ Conv5_x 각각 [3×3 Conv, 64] × 2 블록 (총 16개의 레이어), FC Layer 1000-way Fully Connected 1개 총 18개이다. 각 블록의 채널 수는 64, 128, 256, 512로 늘어난다.
 ![](https://velog.velcdn.com/images/swoo64/post/9db636cd-7163-4b4b-b3ce-5c4d0032b6a6/image.png)
#### 분석 진행
 기존에 학습이 진행된 ResNet18을 파인튜닝한다. 이것은 ImageNet 데이터를 학습했으며, 1000개의 카테고리와 224x224x3의 이미지로 구성되어있다. 이것은 파인튜닝 데이터인 CIFAR-10과 큰 차이가 있다. 따라서 CIFAR-10이미지를 256x256으로 사이즈를 변경한 후 가운데 224x224크기만 사용한다. 또한 ResNet18 학습 때 사용한 표준화와 동일하게 파인튜닝 데이터를 표준화한다. 그리고 마지막 분류기는 1000개의 클래스를 분류하도록 되어있어서, 이것을 10개의 클래스를 예측하도록 수정한다.

학습 없이 inference
 비교군을 만들기 위해 추가 학습 없이 CIFAR-10이미지를 예측해보았다. 그 결과 테스트 정확도는 8.74%가 나왔다. 기존에 학습이 된 데이터가 아니기 때문에 이렇게 저조하게 나왔다. 랜덤 예측 기대치인 10%에도 못미치는 결과이다.

Full Fine-Tuning
 이것은 사전 학습된 모델의 모든 레이어, 파라미터 가중치를 새로운 데이터셋에 맞게 파인튜닝한다. 이것은 처음부터 모델을 학습하는 것이 아닌, 기존의 가중치를 이용하여 미세 조정하는 것이다. 이를 통해 해당 모델은 새 데이터에 완벽히 적응하며 성능을 높일 수 있다. 그러나 학습 시간이 오래 걸리며, 데이터가 적을 경우 과적합이 발생할 수 있다. 손실함수는 크로스엔트로피, 옵티마이저는 AdamW을 사용하였다.  
 총 학습 소요시간은 10에포크에 714.13초가 걸렸으며, 테스트 손실은 0.2336, 테스트 정확도는 92.95%이다. 새로운 데이터를 가지고 기존 모델을 추가학습 시켰을 시 매우 높은 결과를 보여주었다. 

Partial Fine-Tuning
 이것은 기존 모델의 전체가 아닌, 일부 층만 학습시키는 방식이다. 보통 초기층에는 기본적인 시각적 패턴, 후반층에는 구체적인 클래스 특징이 학습되어진다. 뒷부분만 파인튜닝을 진행하여 이미 학습된 저수준의 특징을 그대로 사용, 학습 속도를 빠르게 가져가며 데이터가 적을 경우 과적합도 방지할 수 있다. 
 본 분석에서는 마지막 블록(Conv5_x)과 분류기만 파인튜닝을 진행하고 나머지는 동결(파라미터 재학습 방지)하였다. 총 학습 소요시간은 10에포크에 469.51초가 걸렸으며, 테스트 손실은 0.3496, 테스트 정확도는 90.56%이다. 모델을 전부 학습하는 Full Fine-tuning보다 학습 시간은 획기적을 줄였지만, 테스트 정확도는 낮게 나왔다. 그럼에도 꽤 높은 성능을 보여준다.

Progressive Fine-Tuning
 점진적으로 학습 범위를 늘려가는 방식이다. 처음엔 모델의 상위 레벨만 동결 해제하며 모델이 새 데이터의 패턴을 익히게 한다. 학습이 어느정도 안정되면 일부 하위레이어를 동결 해제하여 파인튜닝을 지속하는 것이다. 처음부터 모든 파라미터를 학습하면 기존의 사전학습 가중치가 무너질 수 있다. 또한 작은 데이터에선 과적합과 손실 폭주가 발생할 수 있다. 이것은 학습을 서서히 늘려가기 때문에 과적합을 억제할 수 있다. 
 본 분석에서는 총 10에포크중 3에포크까지는 분류기만 동결해제, 4에포크부턴 레이어4, 7에포크부턴 레이어3을 동결해제하였다. 총 학습 소요시간은 467.95초이며 테스트 손실은 0.5359, 테스트 정확도는 88.88%이다. 학습 시간은 부분적 파인튜닝과 차이가 없었으며 성능은 보다 안좋다.

#### 결론 
 기존에 학습된 모델을 새로운 데이터에 적용할 시 파인튜닝은 필수적이다. 이때 전체 파인튜닝의 경우 이론상 학습 데이터의 모든 특징을 전부 받아들여 최고의 성능을 보여줄 수 있다. 그러나 학습시 사용되는 자원과 시간이 많이 투입이 되어야 하고, 데이터가 총 6만개로 적은 편이므로 과적합의 위험이 있다. 따라서 일부분만 전이 학습을 진행하는 것을 고려하여야 한다. 이때 얼마만큼의 레이어를 학습에 사용할것인지 선택하여야 한다. 기존 학습 데이터와의 도메인의 차이가 큰 경우, 점진적인 전이 학습이 최적화 효율을 떨어뜨릴 수 있기 때문이다.

### Work 2: VGG16 Fine-Tuning
#### 서론
 이미지 분류 모델 VGG16을 통해 파인튜닝을 진행해본다. 여기선 앞서 좋은 효율과 성능을 보여준 Partial 전이학습과, 분류기 부분만 학습을 하는 Feature Extraction을 서로 비교한다.

#### 데이터셋 설명
 CIFAR-10 데이터셋을 사용한다. 사전학습된 VGG16은 ResNet18과 마찬가지로 이미지넷을 통해 학습되었다. 그러나 지금은 학습 데이터 이미지 크기를 224x224로 변형하지 않고, 원본 사이즈 32x32를 이용한다. 텐서플로로 VGG16을 로드할 때 입력할 데이터의 크기를 사전에 정할 수 있기 때문이다. 따라서 이미지 전처리는 이미지넷 학습 당시의 정규화만 적용한다. 
 데이터의 크기를 유지한채 VGG16에 학습을 하면, 달라진 이미지의 크기로 인하여 입력 해상도가 의도한대로 진행되지 않는다. 기존의 데이터로는 마지막 특징맵의 크기가 7x7이 되지만, CIFAR-10을 그대로 입력할 경우 1x1이 된다. 따라서 마지막 합성곱 블록이 너무 작아져 공간 정보를 거의 잃게 되고 성능이 급격히 저하될 수 있다. 그러나 지금은 최적의 전이 학습을 찾아내는 것이 아닌, 두 전이학습의 효과를 비교할 것이기에 이대로 진행한다.

#### 모델 설명
 VGG16은 3x3 합성곱과 2x2 맥스풀링이 반복되어 깊은 구조를 만든다. 1, 2번 레이어엔 각각 Conv 두개와 하나의 맥스풀링으로 구성되어 있으며, 3, 4, 5번 레이어엔 3개의 Conv와 1개의 맥스풀링으로 이루어져 있다. 완전연결층은 4096개의 뉴런을 가진 FC층 두개와 1000개의 출력을 가진 FC층 하나로 이루어져 있다. 마지막으로 소프트맥스 함수를 통해 1000개의 클래스중 하나를 예측한다. 따라서 총 16개의 층으로 구성된다. 
![](https://velog.velcdn.com/images/swoo64/post/1587cbf9-b759-4a91-91cd-728c11ed2962/image.png)

#### 분석 진행
Feature Extraction
 특징 추출은 사전 학습된 모델의 합성곱층은 그대로 두고, 분류기(FC)를 새롭게 추가한다. 이를 통해 기존의 VGG16이 가진 패턴 인식은 유지할 수 있으며, 적은 데이터로 좋은 성능 및 빠른 학습 속도를 가져갈 수 있다. 또한 CIFAR-10이미지의 크기는 이미지넷보다 크기가 작고 양도 적으므로, 이를 통해 처음부터 학습하면 과적합 발생 및 학습이 불안정해질 수 있다. 특징 추출을 통해 Dense층만 새로 학습하면 해당 문제를 예방할 수 있다.
 본 분석의 경우 VGG16모델을 불러올 때 기존의 분류기는 제거한체 로드했으며, 모든 가중치를 동결시켰다. 이후 GlobalAveragePooling2D으로 각 특징맵을 평균내어 벡터로 변환시켰다. 이것은 기존의 VGG16에는 없는 구조로, Flatten 대신 사용되어 파라미터의 수를 줄이고 과적합을 방지한다. 또한 256 Dense를 통해 가벼운 분류기 헤드를 설계하였으며 0.3 드롭아웃을 추가하여 일반화 성능을 높였다. 마지막으로 클래스 10개 소프트맥스층을 추가하였다. 

Partial Fine-Tuning
Feature Extraction와 동일한 구조를 사용하였으며, 다섯번째 블록만 추가로 동결해제 하였다.

성능 비교표
![](https://velog.velcdn.com/images/swoo64/post/2565066f-310e-48a7-a6b8-678bfa534773/image.png)
합성곱층의 총 파라미터는 두 전이학습 모두 동일하고, 학습을 진행한 파라미터의 수는 Partial 전이학습이 Feature Extraction보다 53.9배 많다. 그에 따라 학습시간도 Partial에서 상당 부분 증가하였다. 하지만 그만큼 검정 결과, 테스트 결과는 더 높은 성능을 보여주었다. 더 많은 레이어에서 학습을 진행하는 Partial 전이학습이 성능에 더 강점을 보여준다.

손실, 정확도 학습 곡선
![](https://velog.velcdn.com/images/swoo64/post/afa1b168-9913-4836-b67d-1d63e6e95b43/image.png)

Feature Extraction보다 Parial Fine-Tuning이 보다 적은 에포크에서 수렴이 되었다. 이것은 학습 효율이 보다 뛰어나다는 뜻이다. 둘 다 검증 손실이 5 에포크 이후 증가하지만, 그 수준은 Partial이 보다 양호하다. 
#### 결론
 Feature Extraction은 분류기의 상단 FC층만, Partial Fine-Tuning은 마지막 합성곱 블록 Conv5_x까지 재학습을 허용하였다. 더 많은 레이어를 학습하여 Partial에서 학습 부하가 걸렸지만, 일반화 성능이 눈에 띄게 향상되었다.

### Work 3: ResNet, VGG16 학습
#### 서론
전이 학습은 기존에 학습된 모델을 이용하는것으로 성능의 향상 및 자원 절약을 기대할 수 있다. 이것과의 차이점을 보기 위해 모델을 처음부터 설계 하여 생성 및 전체 학습을 진행한다.

#### 데이터셋 설명
CIFAR-10 데이터셋을 사용한다. 255로 각 픽셀 값을 나누어 정규화를 진행하였다. 데이터의 크기에 맞추어, 모델도 처음부터 이것을 상정하여 설계한다.

#### 모델 설명
 ResNet32는 직접 설계하였다. Residual Block을 설계하였으며 각 블록은 두개의 Conv층이 들어있다. 초기층에는 1개의 Conv, Stage1 ~ 3은 각각 5개의 블록으로 구성되어있으며 마지막 FC층 1개로 이루어져 있다. 따라서 총 32개의 층으로 구성되어있다.
 VGG16은 기존 이미지넷으로 훈련한 모델의 합성곱층 구조를 그대로 사용하였으며 출력층을 개조하였다. 즉 과제 2의 Feature Extraction과 동일하면서 전체 파라미터를 학습하도록 설정하였다.
#### 분석 진행
성능 비교표
![](https://velog.velcdn.com/images/swoo64/post/9d5b764e-3a55-4f5b-914c-5df4b2b973a5/image.png)
학습에 사용되는 파라미터는 VGG16이 ResNet32보다 높기에 학습 시간도 더 길다. 학습 성능은 모든 면에서 ResNet이 더 높다. 그 이유는, VGG16을 제대로 학습할려면 이미지 증대 및 정규화 방식 변경 등 여러 데이터 전처리가 필수적이다. 따라서 CIFAR-10 데이터 전용 VGG모델을 설계해야만 한다.

손실, 정확도 학습 곡선
![](https://velog.velcdn.com/images/swoo64/post/efbb4c51-fca8-4588-8265-3e00bab404e5/image.png)



### Work 4: GridSearch, RandomSearch
#### 서론
CNN을 이용한 딥러닝에서는 레이어와 뉴런의 크기, 손실함수와 옵티마이저 등 하이퍼파라미터 튜닝이 필요하다. 여기엔 여러 방법이 사용되지만, 그리드 서치와 랜덤서치는 튜닝의 기본적인 방법이며 이 둘을 실습해본다.
#### 데이터셋 설명
CIFAR-10을 사용하였으며 255로 나누어 정규화를 하였다.
#### 모델 설명
기본 베이스로 간단한 CNN모델을 사용하였다. 각 Conv는 3x3커널을 사용하며, Conv 7개와 Dense 2개 총 7개의 학습층으로 구성되어있다.

GridSearch
모델 학습률은 0.001과 0.0005, 배치 사이즈는 64와 128, Dense층 뉴런 수는 128, 256, 512개 중에 하나를 선택한다. 총 12번 모델을 학습한다.

RandomSearch
모델 학습률은 0.001과 0.0001 로그 균등 샘플링, 배치 사이즈는 64, 96, 128, 160 중 하나, Dense층 뉴런 수는 128, 256, 384, 512 중 하나를 선택한다. 총 12번 랜덤선택해야하므로 하이퍼파라미터의 범위를 그리드서치의 그것보다 넓게 설정하였다.

#### 분석 진행
성능 비교표
![](https://velog.velcdn.com/images/swoo64/post/23e50988-10b0-434e-9b8b-f9d07c1761a0/image.png)

테스트 결과는 랜덤서치가 더 높다. 그러나 이것은 그리드서치의 범위를 랜덤서치만큼 넓히지 않았기 때문이다. 하지만 그리 한다면 그만큼 더 많은 시간과 자원을 사용하여야 했으므로 랜덤서치가 더욱 효율적이라고 볼 수 있다.

GridSearch 최적 조합 : 
{'batch_size': 64, 'model__hidden_units': 256, 'model__lr': 0.001}

RandomizedSearch 최적 조합 : 
{'batch_size': 96, 'model__hidden_units': 128, 'model__lr': np.float64(0.0009330606024425672)}

손실, 정확도 학습 곡선
![](https://velog.velcdn.com/images/swoo64/post/4bf9e520-eafa-4a95-8868-7375888b9b47/image.png)


## References and Links
- ![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=GitHub&logoColor=white)
- [7주차과제1_max_ji.ipynb](https://github.com/Max-JI64/Kakao_Tech_Bootcamp/blob/main/Assignment/week7/7%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C1_max_ji.ipynb) 
- [7주차과제2_max_ji.ipynb](https://github.com/Max-JI64/Kakao_Tech_Bootcamp/blob/main/Assignment/week7/7%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C2_max_ji.ipynb)
- [7주차과제3_max_ji.ipynb](https://github.com/Max-JI64/Kakao_Tech_Bootcamp/blob/main/Assignment/week7/7%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C3_max_ji.ipynb)
- [7주차과제4_max_ji.ipynb](https://github.com/Max-JI64/Kakao_Tech_Bootcamp/blob/main/Assignment/week7/7%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C4_max_ji.ipynb)
- ![Google Docs](https://img.shields.io/badge/googledocs-4285F4?style=for-the-badge&logo=googledocs&logoColor=white)
- [7주차과제_보고서_max.ji](https://docs.google.com/document/d/14YELGpiTSVhrRk6hW2Eyju0apoIRhvQffdjMI9ZeaIM/edit?usp=sharing)
- ![velog](https://img.shields.io/badge/Velog-20C997?style=for-the-badge&logo=Velog&logoColor=white)
- [7주차 과제 회고록](https://velog.io/@swoo64/7%EC%A3%BC%EC%B0%A8-%EA%B3%BC%EC%A0%9C-%ED%9A%8C%EA%B3%A0%EB%A1%9D)
