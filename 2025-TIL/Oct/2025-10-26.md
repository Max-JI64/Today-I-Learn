# TIL

## Date: 2025-10-26

## week6 Assignment

### Assignment 1: 활성화 함수
활성화 함수는 모델에 비선형성을 학습할 수 있게 해준다. 이것이 없으면, 인공신경망은 그저 선형함수의 결합일 뿐이다. 이때 활성화 함수를 통해 해당 뉴런의 활성화, 비활성화를 결정하여 비선형성을 구현한다.

#### 시그모이드
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$
![](https://velog.velcdn.com/images/swoo64/post/49e4941a-fa26-4d9b-aecb-57957a6db92c/image.png)

역전파 과정에서 그레디언트 기울기의 중첩은 다음과 같다
![](https://velog.velcdn.com/images/swoo64/post/8ad4c387-1e5b-4410-a42c-77d12c586f05/image.png)

시그모이드는 입력값이 커지거나 작아질수록 1 또는 0에 급격히 수렴한다. 이때 함수의 기울기는 0에 가까워져 포화영역에 들어선다. 역전파 과정에서 한 가중치의 갱신에 사용되는 그레디언트 기울기는 앞선 은닉층의 개수에 비례하여 곱해진다. 따라서 시그모이드 활성화 함수를 사용하면 금세 그 값이 0이 되어 기울기 소실 문제가 발생, 가중치 업데이트가 진행되지 않는다.

#### 하이퍼볼릭 탄젠트
$$\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
![](https://velog.velcdn.com/images/swoo64/post/bf02d157-aa12-4759-81a4-d68fb1de9765/image.png)
![](https://velog.velcdn.com/images/swoo64/post/55a8f1cf-bb85-48ed-992b-5e8642df6750/image.png)

하이퍼볼릭 탄젠트 활성화함수는 시그모이드를 개선할 수 있다.

1. 출력값의 범위가 (-1, 1)이므로, (0, 1)인 시그모이드보다 가중치 학습에 효율성을 가진다.
2. 함수의 기울기의 범위가 (0, 1)이므로 기울기 소실 문제를 완화할 수 있다.

그러나 깊은 신경망에서의 학습인 경우 이것도 기울기 소실 문제를 발생한다.

#### ReLu
$$\text{ReLU}(x) = \max(0, x)$$
![](https://velog.velcdn.com/images/swoo64/post/125b0795-0814-4696-bc13-6741fb342dfa/image.png)

렐루함수는 입력값이 양수면 그대로, 음수면 0을 출력한다. 계산이 단순하여 연산 속도가 빠르고, 양수 영역에서는 기울기가 항상 1이므로 기울기 소실 문제를 개선할 수 있다.
그러나 한번 기울기가 0이되면, 이후의 학습에서 그레디언트 기울기가 전부 0이 되어 죽은 ReLu문제가 발생하여 가중치 업데이트가 진행되지 않는다.

#### Leaky ReLu
$$\text{Leaky ReLU}(x) =
\begin{cases}
x & \text{if } x > 0 \\
\alpha x & \text{if } x \le 0
\end{cases}$$
![](https://velog.velcdn.com/images/swoo64/post/283a189a-c188-4426-ac55-411bbe2e8543/image.png)
리키렐루는 렐루함수에서 입력값이 음수인 경우 작은 수를 입력값에 곱하여 출력한다. 이것은 음수 영역에서 작은 기울기를 허용하여 미세하게나마 뉴런이 학습을 진행할 수 있도록 한다. 그러나 깊은 신경망에선 기울기 소실 문제가 여전히 존재한다.

### Assignment 2: MLP
다층 퍼셉트론은 입력층, 은닉층, 출력층으로 구성되며, 각 층은 퍼셉트론으로 이루어져 있다. 하나의 뉴런은 이전층의 모든 뉴런과 완전히 연결되어 있으며, 입력에 가중치를 곱하고, 편차를 더한 뒤 활성화 함수를 통과시켜 다음 층으로 전달한다. 따라서 해당 모델의 설계와 구성에 따라 그 성능이 크게 달라진다.

이것은 크게 회귀와 분류에 사용되어진다. 회귀인 경우 출력층의 뉴런은 한개이며 출력층의 활성화 함수는 항등함수를 사용한다. 분류인 경우 출력층의 뉴런은 예측 클래스의 개수이며 주로 소프트맥스 활성화 함수를 사용한다.

역전파를 이용하여 모든 뉴런의 가중치를 학습할 수 있다. 이때 사용되는 오차함수는 데이터의 종류와 분석 목적에 따라 다르게 선택되어진다. 또한 적절한 옵티마이저와 학습률도 필요하다.

#### 결과
tensorflow 라이브러리에서 제공하는 make_moons데이터를 사용하여 비선형 데이터 분류 학습을 진행하였다.
![](https://velog.velcdn.com/images/swoo64/post/c9e41b69-7667-4f3c-9d8d-80c603c1adbe/image.png)

예측한 결정 경계의 대표 이미지는 다음과 같다.
![](https://velog.velcdn.com/images/swoo64/post/e69941c2-9f26-4c5f-bee2-07176e82f226/image.png)

여러 모델의 뉴런과 입력층의 수를 추가하고, 옵티마이저와 활성화 함수를 변환하였다. 그 결과, 사전지식으로 성능이 더 좋은 것들을 추가하였음에도 모델의 성능이 드라마틱하게 좋아지진 않았다. 해당 데이터가 구조적으로 분류하기 힘든 것도 있지만, 오히려 성능이 떨어지는 경우도 발생하였다.

모델을 복잡하게 만들수록 복잡한 패턴을 학습할 수 있지만, 그만큼 노이즈도 학습하여 예측 결과가 안좋아질 가능성도 높아진다. 따라서 주어진 데이터셋에 적합한 모델을 찾아나서는 것이 중요하다.

### Assignment 3: CNN
이미지는 합성곱 신경망으로 학습을 진행한다. 이것은 크게 합성곱 Conv, 풀링 Pool, 전결합 Dense 계층으로 구성되어있다.
 Conv는 이미지의 각 영역에 필터를 적용하여 테두리와 같은 중요한 특징을 추출한다. Pool은 각 구역의 대푯값(최대치, 평균)을 추출하여 차원 축소를 진행하여 데이터를 압축한다. Conv와 Pool은 은닉층의 각 층을 구성한다. 마지막으로 데이터를 1차원으로 변환하고 Dense 전결합층에 전달하여 최종 결과를 생성한다.

이미지 분석에는 CNN이 매우 큰 장점을 가진다. 픽셀 값의 배열이 아니라 이미지의 공간적인 특징을 자동적으로 추출하여 분석 성능을 높인다. 이때 초기엔 테두리, 색상 대비 등 단순한 특징을 학습하며, 후엔 심층 학습망을 거치며 단순 특징들을 조합, 손, 발. 원 등 복잡하고 추상적인 개념을 학습할 수 있다.
 또한 CNN은 전결합 신경망 FCN보다 사용하는 파라미터의 수가 적어 과적합의 위험을 줄이고 사용되는 자원도 절약할 수 있다.


## References and Links
- ![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=GitHub&logoColor=white)
- [2025-10-26.md](https://github.com/Max-JI64/Today-I-Learn/blob/main/2025-TIL/Oct/2025-10-26.md) 
- [6주차과제1_max_ji.ipynb](https://github.com/Max-JI64/Kakao_Tech_Bootcamp/blob/main/Assignment/week6/6%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C1_max_ji.ipynb) 
- [6주차과제2_max_ji.ipynb](https://github.com/Max-JI64/Kakao_Tech_Bootcamp/blob/main/Assignment/week6/6%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C2_max_ji.ipynb)
- [6주차과제3_max_ji.ipynb](https://github.com/Max-JI64/Kakao_Tech_Bootcamp/blob/main/Assignment/week6/6%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9C3_max_ji.ipynb)
- ![Google Docs](https://img.shields.io/badge/googledocs-4285F4?style=for-the-badge&logo=googledocs&logoColor=white)
- [6주차과제_보고서_max.ji](https://docs.google.com/document/d/1S01gNpvrnsJxfmYs0WVTcJYhlf-Xpyfh5UoP1TnCV24/edit?usp=sharing)
- ![velog](https://img.shields.io/badge/Velog-20C997?style=for-the-badge&logo=Velog&logoColor=white)
- [6주차 과제 회고록](https://velog.io/@swoo64/6%EC%A3%BC%EC%B0%A8-%EA%B3%BC%EC%A0%9C-%ED%9A%8C%EA%B3%A0%EB%A1%9D)
