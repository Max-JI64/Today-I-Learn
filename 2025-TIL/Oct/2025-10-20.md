# TIL

## Date: 2025-10-20

## Today What I Learn
### Learn 1 : Deep Learning
대량의 데이터를 기반으로 비선형 모델을 자동으로 만들어주는 기법이다.
머신러닝은 알고리즘을 선택 후 훈련하여 하이퍼파라미터를 조정하지만, 딥러닝은 신경망 구조를 직접 설계하며 가중치와 바이어스를 최적화한다.
### Learn 2 : Perceptron
ANN의 기본 단위로 입력값을 가중치와 함께 처리하여 단일 출력을 생성하는 선형 이진 분류기이다.  
이것은 weighted sum(가중합)과 activation function으로 구성되어 있다.  
가중합은 입력값과 가중치의 선형 결합을 의미한다. (입력값 * 가중치 + 편향)
활성화 함수의 종류에 따라 출력값은 특정 범위로 변환되며, 이진 분류 문제에서는 주로 0과 1과 같은 이진값으로 해석된다.

### Learn 3 : Activation Function (비선형 활성화 함수)
인공신경망에서 뉴런의 출력을 결정하는 비선형 함수이다. 노드가 입력신호를 받아 가중합을 계산한 후 비선형 함수에 적용하여 최종 출력을 생성한다.  
이것은 신경망의 학습 능력과 예측 성능에 큰 영향을 미치며, 네트워크의 복잡한 패턴 인식을 가능하게 한다.
여기엔 시그모이드, 하이퍼볼릭 탄젠트, 렐루 함수 등이 있다.

### Learn 4 : Artificial Neural Network (ANN)
인공적으로 생물학적 뉴런 신경 회로망을 모방한 것을 말한다.  
이것은 가중치를 학습하는데 크게 Feed-Forward, Loss Function, Backpropagation, Gradient Descent를 사용한다.
### Learn 5 : Fully Connected Layer
인공신경망에서 모든 입력 뉴런이 모든 출력 뉴련과 연결된 레이어이다.  
이것은 입력 데이터의 모든 정보를 종합적으로 분석하고 학습한다.  
그러나 학습해야 할 파라미터의 수가 기하급수적으로 증가한다.  
또한 특정 데이터 패턴을 지나치게 학습하거나 불필요한 학습의 가능성이 증가하여 과적합의 문제가 발생할 수 있다.
### Learn 6 : Loss Function
Loss Function은 인공신경망이나 머신러닝 모델에서 예측값과 실제값 간의 차이를 정량적으로 측정하는 함수이다. 모델은 예측 오차를 최소화 하는 방향으로 학습한다.  
- Regression : MSE, MAE
- Classification : Cross-Entropy Loss, Hinge Loss(주로 SVM에서 사용)

### Learn 7 : Gradient Descent
Gradient Descent는 머신러닝과 딥러닝에서 Loss Function를 최소화하기 위해 가중치를 반복적으로 조정하는 Optimization Algorithm이다. 이것은 Loss Function의 기울기를 계산하여 가중치를 기울기의 반대 방향으로 업데이트함으로써 손실 값을 점진적으로 줄여나간다.

### Learn 8 : Optimizer
딥러닝 모델의 손실 함수를 최소화하기 위해 기울기를 기반으로 가중치를 업데이트 하는 알고리즘
![Optimizer image](https://velog.velcdn.com/images/swoo64/post/7609f940-3176-43fe-a6f9-488ecc878ced/image.png)
- Gradient Descent  
배치 경사 하강법 : 전체 데이터셋을 사용하여 한번에 가중치를 업데이트, 경로가 곧고 흔들림이 없다  
확률적 경사 하강법 : 하나의 데이터 샘플을 사용하여 가중치를 업데이트, 경로가 요동치며 불안정  
미니 배치 경사 하강법 : 데이터셋을 작은 배치로 나누어 각 배치마다 가중치를 업데이트, 어느정도 흔들리지만 전체적으로 빠르고 안정적으로 수렴  
![Gradient image](https://velog.velcdn.com/images/swoo64/post/f2b2586a-01c6-490b-aaea-0ae7c333cfa7/image.png)
  
- Adaptive Optimizers  
학습률을 동적으로 조정하여 학습효율을 높여주는 옵티마이저  

- Momentum Optimizers  
기울기 벡터의 지수 이동 평균을 사용하여 가중치를 업데이트  
지수 이동 평균(EMA)는 최근의 기울기 변화에 더 큰 가중치를 부여하고, 과거의 기울기 변화를 지수적으로 감소시킨다, 기울기 벡터의 변동성을 줄여주기 때문에 SGD의 단점을 보완

### Learn 9 : Backpropagation


## Today I Work
### Work 1:


## Today's Assignment and How to Solve
### Assignment 1 :

## Today's Memoirs
- 오늘의 학습 경험에 대한 자유로운 생각이나 느낀 점을 기록합니다.
- 성공적인 점, 개선해야 할 점, 새롭게 시도하고 싶은 방법 등을 포함할 수 있습니다.

## References and Links
- ![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=GitHub&logoColor=white) 
- [2025-10-20.md](https://github.com/100-hours-a-week/max-til/blob/main/Oct/2025-10-20.md) 
- ![GitHub](https://img.shields.io/badge/googlecolab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white) 
- [ANN](https://drive.google.com/file/d/1g9nIqC4rvmIUKkGX8oUIYdIjAG90og7B/view?usp=sharing) 
- [Perceptron](https://drive.google.com/file/d/1EOaSjccjEiqiR5GqfdGDnxUML6AmrlUv/view?usp=sharing) 
- [Activation Function](https://drive.google.com/file/d/1id0B5z3G9iixj9Dxb-uEEdY5Lsh-M0m1/view?usp=sharing) 
- [Loss Function](https://drive.google.com/file/d/1WSWldfa4R9f0xxSgjcJpi4M1eI8bNJ3O/view?usp=sharing) 
- [Adam](https://drive.google.com/file/d/1OMaLkh9yDtoGbd6zyLJtqaCtijzvWrYh/view?usp=sharing) 
- [Fully Connected Layer](https://drive.google.com/file/d/1cBsAC3ZZJxTHDF207GGxbyly25XIRy1w/view?usp=sharing) 
- [Fully Connected Neural Network](https://drive.google.com/file/d/1hllAqhFi79Su4P7kzkS3bB1I_OUvWAJg/view?usp=sharing) 
- [Backpropagation](https://drive.google.com/file/d/1G-9wErTQrldZI5fUnb-SRBXb7l7ocgam/view?usp=sharing) 

